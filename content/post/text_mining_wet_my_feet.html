---
title: Getting myself started into text mining
author: Rushad Faridi
date: '2018-03-07'
slug: 'intro-text-mining'
categories: []
tags: []
---



<p>I will be just practicing the exercises in the vignette of tidytext package.</p>
<p>let’s first import the required libraries:</p>
<pre class="r"><code>library(janeaustenr)
library(dplyr)</code></pre>
<pre><code>## 
## Attaching package: &#39;dplyr&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:stats&#39;:
## 
##     filter, lag</code></pre>
<pre><code>## The following objects are masked from &#39;package:base&#39;:
## 
##     intersect, setdiff, setequal, union</code></pre>
<pre class="r"><code>library(stringr)
library(tidytext)</code></pre>
<p>As the very first step, we create line number variable and chapter variable. We do this for each separate book.</p>
<pre class="r"><code>original_books  &lt;- austen_books()  %&gt;% 
                                group_by(book) %&gt;% 
                mutate(linenumber= row_number(),
                       chapter=cumsum(str_detect(text,regex(&quot;^chapter [\\divxlc]&quot;, 
                                        ignore.case=TRUE)))) %&gt;% 
                       ungroup()</code></pre>
<p>Now we need to have one word per row to work in a tidy data frame. For this to work we will use <code>unnest_token</code> function from <code>tidytext</code> package. That means, it will break each sentenced into words and put a word in each row. Currently <code>original_books</code> data frame has 73422 number of rows. We expect to increase it many fold. Let’s check this out.</p>
<pre class="r"><code>tidy_books  &lt;- original_books %&gt;% 
        unnest_tokens(word, text) </code></pre>
<p>Now we are going to remove the <code>stop words</code> meaning those words which are used repeatedly without any special meaning such <code>and</code>, <code>there</code> etc.</p>
<pre class="r"><code>clean_books  &lt;- tidy_books %&gt;% 
                 anti_join(stop_words)</code></pre>
<pre><code>## Joining, by = &quot;word&quot;</code></pre>
<pre class="r"><code>save(clean_books, file=&quot;../RDATA/clean_books.RData&quot;)</code></pre>
<p>We can try to find the most common words used in the following manner:</p>
<pre class="r"><code>clean_books %&gt;% 
    count(word, sort=TRUE)</code></pre>
<pre><code>## # A tibble: 13,914 x 2
##      word     n
##     &lt;chr&gt; &lt;int&gt;
##  1   miss  1855
##  2   time  1337
##  3  fanny   862
##  4   dear   822
##  5   lady   817
##  6    sir   806
##  7    day   797
##  8   emma   787
##  9 sister   727
## 10  house   699
## # ... with 13,904 more rows</code></pre>
<p>Now let’s go straigh into creating a word cloud. But before that let’s install the wordcloud package.</p>
<pre class="r"><code>library(wordcloud)</code></pre>
<pre><code>## Loading required package: RColorBrewer</code></pre>
<p>Now time for word cloud:</p>
<pre class="r"><code>clean_books %&gt;% 
    count(word) %&gt;% 
    with(wordcloud(word,n, max.words=100))</code></pre>
<p><img src="/post/text_mining_wet_my_feet_files/figure-html/wc2-1.png" width="672" /></p>
<p>That’s it! my first ever word cloud in R!</p>
